{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "mount_file_id": "10Evz0KfAM2YxE5M1MUfsZiLmWLuNenF6",
      "authorship_tag": "ABX9TyOXdGf+u/+nqRAAYXSLmZaP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PraveenSH/adversarial-generator-retriever/blob/main/generator_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7LftNzVazZY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Generator"
      ],
      "metadata": {
        "id": "rtmv2pVKa_Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"google-t5/t5-base\"\n",
        "\n",
        "gen_tokenizer = T5Tokenizer.from_pretrained(MODEL_ID)\n",
        "gen_model = T5ForConditionalGeneration.from_pretrained(MODEL_ID)\n",
        "\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/Generator_Retriever/Generator_Pretrained/model.pt_1_2000\")\n",
        "gen_model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IcjLAgx6a-el",
        "outputId": "adf53780-615f-4e55-a0af-f219a7a8ef97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "gen_model.eval()\n",
        "\n",
        "prod_title = \"coelia ombre gray area rug\"\n",
        "prod_desc = \"woven on state of the art machines for ultimate durability and luxury , this rug is an elegant and sophisticated solution for your design needs . the rug is        crafted with deliciously soft viscose fibers on a soft cotton weft for unbeatable comfort underfoot . the rug features a high sheen , plush silken touch with intricate layers of color which shift beautif       ully depending on the angle from which you view the rug . the subtle distressing effect adds on trend , vintage appeal . shedding of loose fibers is normal with this construction . this will diminish with regular vacuuming and wear\"\n",
        "input_sequence = \"Title: \" + prod_title + \" - Description: \" + prod_desc\n",
        "\n",
        "input_ids = gen_tokenizer([input_sequence, input_sequence], return_tensors=\"pt\").input_ids\n",
        "print(input_ids.size())\n",
        "print(f'Input: {input_sequence}')\n",
        "\n",
        "nsent = 4\n",
        "with torch.no_grad():\n",
        "    for i in range(nsent):\n",
        "        output = gen_model.generate(input_ids, max_length=35, num_beams=1, do_sample=True, repetition_penalty=1.8)\n",
        "        for i in range(len(output)):\n",
        "          target_sequence = gen_tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "          print(f'Target: {target_sequence}')\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "collapsed": true,
        "id": "ORa4FD5Lbea-",
        "outputId": "e3df43cc-58c3-4080-a60f-7057b6ffd83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ngen_model.eval()\\n\\nprod_title = \"coelia ombre gray area rug\"\\nprod_desc = \"woven on state of the art machines for ultimate durability and luxury , this rug is an elegant and sophisticated solution for your design needs . the rug is        crafted with deliciously soft viscose fibers on a soft cotton weft for unbeatable comfort underfoot . the rug features a high sheen , plush silken touch with intricate layers of color which shift beautif       ully depending on the angle from which you view the rug . the subtle distressing effect adds on trend , vintage appeal . shedding of loose fibers is normal with this construction . this will diminish with regular vacuuming and wear\"\\ninput_sequence = \"Title: \" + prod_title + \" - Description: \" + prod_desc\\n\\ninput_ids = gen_tokenizer([input_sequence, input_sequence], return_tensors=\"pt\").input_ids\\nprint(input_ids.size())\\nprint(f\\'Input: {input_sequence}\\')\\n\\nnsent = 4\\nwith torch.no_grad():\\n    for i in range(nsent):\\n        output = gen_model.generate(input_ids, max_length=35, num_beams=1, do_sample=True, repetition_penalty=1.8)\\n        for i in range(len(output)):\\n          target_sequence = gen_tokenizer.decode(output[i], skip_special_tokens=True)\\n          print(f\\'Target: {target_sequence}\\')\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Retriever"
      ],
      "metadata": {
        "id": "0X9KtlD3eO7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"google-bert/bert-large-uncased\"\n",
        "ret_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "ret_model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/Generator_Retriever/Retriever_Pretrained/model.pt_1_1000\"\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "ret_model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bj3fOfD-cW09",
        "outputId": "58eba2cf-68ac-40ff-d50b-3154550e5bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "ret_model.eval()\n",
        "import torch.nn.functional as F\n",
        "query_text = \"horizontal coastal look chair\"\n",
        "prod_text = \"laplante beveled accent mirror  showcasing a casual coastal look , this square mirror is finished in a driftwood tone with a distressed whitewash . mirror has a generous 1 1/4 '' bevel and may be hung horizontal or vertical\"\n",
        "tok_output = ret_tokenizer(query_text, prod_text, padding='max_length',\n",
        "                      max_length=160, truncation=True, return_tensors='pt',\n",
        "                      return_attention_mask=True)\n",
        "\n",
        "tok_output = ret_tokenizer([query_text, \"query_text\"], [prod_text, \"prod_text\"], padding='max_length',\n",
        "                      max_length=160, truncation=True, return_tensors='pt',\n",
        "                      return_attention_mask=True)\n",
        "print(tok_output)\n",
        "\n",
        "input_ids = tok_output.input_ids\n",
        "attention_masks = tok_output.attention_mask\n",
        "token_type_ids = tok_output.token_type_ids\n",
        "\n",
        "output = ret_model(input_ids, attention_mask = attention_masks,\n",
        "                   token_type_ids = token_type_ids, labels = torch.tensor([1, 0]))\n",
        "print(output.loss)\n",
        "probs = F.softmax(output.logits, dim=-1)\n",
        "probs[:,1]\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "collapsed": true,
        "id": "9cvUY4MxeaJQ",
        "outputId": "641eb15a-31c0-4f15-8372-b27580d3682e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nret_model.eval()\\nimport torch.nn.functional as F\\nquery_text = \"horizontal coastal look chair\"\\nprod_text = \"laplante beveled accent mirror  showcasing a casual coastal look , this square mirror is finished in a driftwood tone with a distressed whitewash . mirror has a generous 1 1/4 \\'\\' bevel and may be hung horizontal or vertical\"\\ntok_output = ret_tokenizer(query_text, prod_text, padding=\\'max_length\\',\\n                      max_length=160, truncation=True, return_tensors=\\'pt\\',\\n                      return_attention_mask=True)\\n\\ntok_output = ret_tokenizer([query_text, \"query_text\"], [prod_text, \"prod_text\"], padding=\\'max_length\\',\\n                      max_length=160, truncation=True, return_tensors=\\'pt\\',\\n                      return_attention_mask=True)\\nprint(tok_output)\\n\\ninput_ids = tok_output.input_ids\\nattention_masks = tok_output.attention_mask\\ntoken_type_ids = tok_output.token_type_ids\\n\\noutput = ret_model(input_ids, attention_mask = attention_masks,\\n                   token_type_ids = token_type_ids, labels = torch.tensor([1, 0]))\\nprint(output.loss)\\nprobs = F.softmax(output.logits, dim=-1)\\nprobs[:,1]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a relevance model"
      ],
      "metadata": {
        "id": "YhCztsHD_z90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers==2.7.0\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "rel_model = SentenceTransformer('BlueAvenir/sustainability_relevance_class_model')\n",
        "#rel_model.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "22Uxy-e7fEll",
        "outputId": "bf5665e5-33d1-4dc5-8ebf-6a9b3093ca3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers==2.7.0 in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.7.0) (4.41.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.7.0) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.7.0) (2.3.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.7.0) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.7.0) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.7.0) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.7.0) (0.23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.7.0) (10.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (2024.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==2.7.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==2.7.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers==2.7.0) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers==2.7.0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers==2.7.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers==2.7.0) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==2.7.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==2.7.0) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers==2.7.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers==2.7.0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers==2.7.0) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b2E3Nx9y4SB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_sim(q_embeddings, p_embeddings):\n",
        "  return (cosine_similarity(q_embeddings.reshape(1, -1), p_embeddings.reshape(1, -1))[0][0] + 1) / 2\n",
        "\n",
        "  sims = []\n",
        "  all_sims = cosine_similarity(q_embeddings, p_embeddings)\n",
        "  for i in range(len(q_embeddings)):\n",
        "    sims.append(all_sims[i][i])\n",
        "  return sims\n",
        "\n",
        "queries = \"driftwood mirror\"\n",
        "prods = \"laplante beveled accent mirror  showcasing a casual coastal look , this square mirror is finished in a driftwood tone with a distressed whitewash . mirror has a generous 1 1/4 '' bevel and may be hung horizontal or vertical\"\n",
        "q_embeddings = rel_model.encode(queries)\n",
        "p_embeddings = rel_model.encode(prods)\n",
        "\n",
        "print(get_sim(p_embeddings, q_embeddings))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "SxuDej87AIme",
        "outputId": "e43985a6-f868-4c88-beb1-b7b181529746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n\\ndef get_sim(q_embeddings, p_embeddings):\\n  return (cosine_similarity(q_embeddings.reshape(1, -1), p_embeddings.reshape(1, -1))[0][0] + 1) / 2\\n\\n  sims = []\\n  all_sims = cosine_similarity(q_embeddings, p_embeddings)\\n  for i in range(len(q_embeddings)):\\n    sims.append(all_sims[i][i])\\n  return sims\\n\\nqueries = \"driftwood mirror\"\\nprods = \"laplante beveled accent mirror  showcasing a casual coastal look , this square mirror is finished in a driftwood tone with a distressed whitewash . mirror has a generous 1 1/4 \\'\\' bevel and may be hung horizontal or vertical\"\\nq_embeddings = rel_model.encode(queries)\\np_embeddings = rel_model.encode(prods)\\n\\nprint(get_sim(p_embeddings, q_embeddings))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def get_relevance_reward(rel_model, query, product):\n",
        "\n",
        "  q_emb = rel_model.encode(query)\n",
        "  p_emb = rel_model.encode(product)\n",
        "\n",
        "  sim = cosine_similarity(q_emb.reshape(1, -1), p_emb.reshape(1, -1))[0][0]\n",
        "  return sim\n",
        "  #return (sim + 1) / 2"
      ],
      "metadata": {
        "id": "2vDUTyNyXSQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_generator_retriever(dataloader, generator, retriever, relevance_model,\n",
        "                              gen_optimizer, ret_optimizer, gen_scheduler,\n",
        "                              gen_tokenizer, ret_tokenizer,\n",
        "                              ret_scheduler, device, epoch_num):\n",
        "  ret_total_loss = 0\n",
        "  gen_total_loss = 0\n",
        "  steps = 0\n",
        "\n",
        "  for batch in dataloader:\n",
        "   for it in range(2):\n",
        "    steps += 1\n",
        "    generator.eval()\n",
        "    #Generator generates k queries for each sample in the batch. New batch of (batch*k)\n",
        "    prod_input_ids = batch[\"input_ids\"]\n",
        "    gen_query_ids = generator.generate(prod_input_ids, max_length=35, num_beams=1,\n",
        "                                do_sample=True, top_k=0, repetition_penalty=1.8)\n",
        "    gen_queries = []\n",
        "    prod_descs = []\n",
        "    neg_queries = batch[\"neg_queries\"]\n",
        "\n",
        "    for i in range(len(gen_query_ids)):\n",
        "      gen_sequence = gen_tokenizer.decode(gen_query_ids[i], skip_special_tokens=True)\n",
        "      prod_sequence = gen_tokenizer.decode(prod_input_ids[i], skip_special_tokens=True)\n",
        "\n",
        "      gen_queries.append(gen_sequence)\n",
        "      prod_descs.append(prod_sequence)\n",
        "\n",
        "    rewards = []\n",
        "    #Collect losses of retriever/ reward for generator\n",
        "    for i in range(len(gen_queries)):\n",
        "\n",
        "        retriever.eval()\n",
        "        query_text = gen_queries[i]\n",
        "        prod_text = prod_descs[i]\n",
        "        neg_query = neg_queries[i]\n",
        "        tok_output = ret_tokenizer(query_text, prod_text, padding='max_length',\n",
        "                        max_length=160, truncation=True, return_tensors='pt',\n",
        "                        return_attention_mask=True)\n",
        "\n",
        "\n",
        "        input_ids = tok_output.input_ids\n",
        "        attention_masks = tok_output.attention_mask\n",
        "        token_type_ids = tok_output.token_type_ids\n",
        "\n",
        "        output = retriever(input_ids, attention_mask = attention_masks,\n",
        "                          token_type_ids = token_type_ids)\n",
        "        #reward for generator - score of irrelevant class -> maximize\n",
        "        ret_reward = (F.softmax(output.logits, dim=-1)[0][0] - 0.5) * 2.0\n",
        "\n",
        "\n",
        "\n",
        "        #Inner loop train retriever with above.\n",
        "        retriever.train()\n",
        "        output_pos = retriever(input_ids, attention_mask = attention_masks,\n",
        "                           token_type_ids = token_type_ids,\n",
        "                           labels = torch.tensor([1]))\n",
        "\n",
        "        tok_output = ret_tokenizer(neg_query, prod_text, padding='max_length',\n",
        "                        max_length=160, truncation=True, return_tensors='pt',\n",
        "                        return_attention_mask=True)\n",
        "        input_ids = tok_output.input_ids\n",
        "        attention_masks = tok_output.attention_mask\n",
        "        token_type_ids = tok_output.token_type_ids\n",
        "\n",
        "        output_neg = retriever(input_ids, attention_mask = attention_masks,\n",
        "                           token_type_ids = token_type_ids,\n",
        "                           labels = torch.tensor([0]))\n",
        "\n",
        "        ret_loss = output_pos.loss + output_neg.loss\n",
        "        ret_total_loss += ret_loss\n",
        "\n",
        "        ret_optimizer.zero_grad()\n",
        "        ret_loss.backward()\n",
        "        ret_optimizer.step()\n",
        "        ret_scheduler.step()\n",
        "\n",
        "        rel_reward = get_relevance_reward(relevance_model, query_text, prod_text)\n",
        "\n",
        "        alpha = 0.4\n",
        "        final_reward = alpha * rel_reward + (1 - alpha) * ret_reward\n",
        "        #if i < 5:\n",
        "        # print(it, query_text, ret_reward, rel_reward, prod_text)\n",
        "\n",
        "        rewards.append(final_reward)\n",
        "\n",
        "    #Update Generator with losses and relevance as reward. -> policy gradient loss\n",
        "    generator.train()\n",
        "    logits = generator(prod_input_ids, labels = gen_query_ids).logits\n",
        "    log_probs = torch.log(F.softmax(logits, dim=-1))\n",
        "    gathered_log_probs = torch.gather(log_probs, 2, gen_query_ids.unsqueeze(-1)).squeeze()\n",
        "    reward_tensor = torch.tensor(rewards).view(-1, 1)\n",
        "    gen_loss = -torch.mul(gathered_log_probs, reward_tensor).sum(dim=-1).mean()\n",
        "    #print(gen_loss)\n",
        "    gen_total_loss += gen_loss\n",
        "\n",
        "    gen_optimizer.zero_grad()\n",
        "    gen_loss.backward()\n",
        "    gen_optimizer.step()\n",
        "    gen_scheduler.step()\n",
        "\n",
        "    if ((steps%log_steps) == 0):\n",
        "            print(steps, ret_total_loss/steps, gen_total_loss/steps)\n",
        "\n",
        "    if ((steps%save_steps) == 0):\n",
        "          torch.save({'model_state_dict': gen_model.state_dict(), 'optimizer_state_dict': gen_optimizer.state_dict()}, gen_save_path+\"_\"+str(epoch_num)+\"_\"+str(steps))\n",
        "          torch.save({'model_state_dict': ret_model.state_dict(), 'optimizer_state_dict': ret_optimizer.state_dict()}, ret_save_path+\"_\"+str(epoch_num)+\"_\"+str(steps))\n"
      ],
      "metadata": {
        "id": "0OnWaifI_9UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t')\n",
        "    results = df[['query', 'product_name', 'product_description']].fillna(\"\").values.tolist()\n",
        "    return results\n",
        "\n",
        "class ProductDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        query, product_name, product_description = self.data[index]\n",
        "        input_text = \"Title: \" + product_name + \" - Description: \" + product_description\n",
        "\n",
        "        input_ids = self.tokenizer(input_text, padding='max_length', max_length=max_prod_len, truncation=True, return_tensors='pt').input_ids\n",
        "        return {'input_ids': input_ids[0], 'neg_queries': query}"
      ],
      "metadata": {
        "id": "OX3dlsj5f7OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_save_path = \"/content/drive/MyDrive/Generator_Retriever/Generator/model.pt\"\n",
        "ret_save_path = \"/content/drive/MyDrive/Generator_Retriever/Retriever/model.pt\"\n",
        "data_path = \"/content/drive/MyDrive/WANDS/non_pretrain_neg.csv\"\n",
        "max_query_len = 35\n",
        "max_prod_len = 128\n",
        "log_steps = 10\n",
        "save_steps = 30\n",
        "batch_size = 8\n",
        "num_epochs = 4\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = \"cpu\"\n",
        "\n",
        "data = load_dataset(data_path)\n",
        "import random\n",
        "random.shuffle(data)\n",
        "\n",
        "dataset = ProductDataset(data[:5000], gen_tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "gen_optimizer = AdamW(gen_model.parameters(), lr=1e-5)\n",
        "gen_scheduler = get_linear_schedule_with_warmup(gen_optimizer, num_warmup_steps=200,\n",
        "                                                num_training_steps = num_epochs * (len(dataset) // batch_size))\n",
        "\n",
        "ret_optimizer = AdamW(ret_model.parameters(), lr=2e-5)\n",
        "ret_scheduler = get_linear_schedule_with_warmup(ret_optimizer, num_warmup_steps=200,\n",
        "                                                num_training_steps = num_epochs * (len(dataset) // batch_size))\n",
        "\n",
        "for epoch_num in range(num_epochs):\n",
        "    train_generator_retriever(dataloader, gen_model, ret_model, rel_model,\n",
        "                                  gen_optimizer, ret_optimizer, gen_scheduler,\n",
        "                                  gen_tokenizer, ret_tokenizer,\n",
        "                                  ret_scheduler, device, epoch_num)"
      ],
      "metadata": {
        "id": "Ze7RRimeWYpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564c33f2-648f-431c-a10e-e541abae4eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 tensor(3.9572, grad_fn=<DivBackward0>) tensor(-17.3024, grad_fn=<DivBackward0>)\n",
            "20 tensor(3.9348, grad_fn=<DivBackward0>) tensor(-17.9963, grad_fn=<DivBackward0>)\n",
            "30 tensor(4.0126, grad_fn=<DivBackward0>) tensor(-15.6529, grad_fn=<DivBackward0>)\n",
            "40 tensor(3.8984, grad_fn=<DivBackward0>) tensor(-16.1881, grad_fn=<DivBackward0>)\n",
            "50 tensor(4.5095, grad_fn=<DivBackward0>) tensor(-14.5680, grad_fn=<DivBackward0>)\n",
            "60 tensor(4.7724, grad_fn=<DivBackward0>) tensor(-13.6612, grad_fn=<DivBackward0>)\n",
            "70 tensor(4.8400, grad_fn=<DivBackward0>) tensor(-13.1678, grad_fn=<DivBackward0>)\n",
            "80 tensor(4.8534, grad_fn=<DivBackward0>) tensor(-13.2707, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5mqqtRkl_Xv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}